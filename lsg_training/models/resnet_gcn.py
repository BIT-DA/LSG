import torch
import torch.nn as nn
from torch.nn import Parameter
import torch.utils.model_zoo as model_zoo
import math

__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']

# Augment adjacency matrix as follows:

#   A_text | A_ti
#  ---------------
#   A_ti.T | A_img

def gen_Aall(A_text, l_text, l_img, w_ti, w_ii):
    A_ti = torch.eq(l_text.unsqueeze(1),l_img.unsqueeze(1).T).float() * w_ti
    eq_img = torch.eq(l_img.unsqueeze(1),l_img.unsqueeze(1).T).float()
    mask_img = torch.eye(l_img.size(0)).cuda()
    A_img = mask_img + (1-mask_img) * eq_img * w_ii
    A_t = torch.cat((A_text, A_ti), dim=1)
    A_i = torch.cat((A_ti.T,A_img), dim=1)
    A_all = torch.cat((A_t,A_i), dim=0)
    return A_all

# Generate adjacency matrix for LSG
def gen_A2(features, ratio=0.002): #v2: generated by ratio
    ratio = 1 - ratio
    features_norm = features / features.norm(dim=-1,keepdim=True)
    _adj = features_norm @ features_norm.T
    _adj_flat = _adj.view(-1)
    th, _ = torch.kthvalue(_adj_flat, int(ratio*_adj_flat.size(0))) #top k-th value of the full similarity matrix
    similar = (_adj >= th).float()
    above = similar.sum()
    below = (1-similar).sum()
    
    print("Connected edges {}, unconnected edges {}, ratio {:.1f}".format(above, below, above*100/(above+below)))
    return similar*_adj

def gen_A2Plus(features, ratio=0.002): #v2Plus: generated by ratio, always keep the edges connecting the same label
    nprompts = 20
    ncls = int(features.size(0)/nprompts) 
    mask = 1 - gen_A_gt(ncls, nprompts)

    ratio = 1 - ratio
    features_norm = features / features.norm(dim=-1,keepdim=True)
    _adj = features_norm @ features_norm.T
    _adj_m = _adj * mask
    _adj_flat = _adj_m.view(-1)
    th, _ = torch.kthvalue(_adj_flat, int(ratio*_adj_flat.size(0))) #top k-th value of the full similarity matrix
    similar = (_adj >= th).float()
    above = similar.sum()
    below = (1-similar).sum()

    print("Connected edges {}, unconnected edges {}, ratio {:.3f}".format(above, below, above*100/(above+below)))
    similar = similar * mask + (1-mask)
    return similar*_adj

def gen_A_gt(ncls,nprompts):
    a = torch.ones(nprompts,nprompts)
    a_all = a.unsqueeze(0).repeat(ncls,1,1)
    return torch.block_diag(*a_all).cuda()

def gen_adj(A):
    D = torch.pow(A.sum(1).float(), -0.5)
    D = torch.diag(D)
    adj = torch.matmul(torch.matmul(A, D).t(), D)
    return adj

class GraphConvolution(nn.Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, bias=False):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.Tensor(in_features, out_features))
        # print("bias",bias)
        if bias:
            self.bias = Parameter(torch.Tensor(1, 1, out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
        # print(self.weight.data)

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = torch.matmul(input, self.weight)
        output = torch.matmul(adj, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'


model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class Normalize(nn.Module):

    def __init__(self, power=2):
        super(Normalize, self).__init__()
        self.power = power

    def forward(self, x):
        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)
        out = x.div(norm)
        return out


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, text_features, text_labels, in_channel=3,  width=1, edge_ratio=0.002):
        self.inplanes = 64
        n_class, n_prompt, text_dim = text_features.size()
        self.text_dim = text_dim
        self.text_features = text_features.reshape(-1, self.text_dim).cuda()
        self.n_texts = self.text_features.size(0)
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)

        self.base = int(64 * width)

        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, self.base, layers[0])
        self.layer2 = self._make_layer(block, self.base * 2, layers[1], stride=2)
        self.layer3 = self._make_layer(block, self.base * 4, layers[2], stride=2)
        self.layer4 = self._make_layer(block, self.base * 8, layers[3], stride=2)
        self.avgpool = nn.AvgPool2d(7, stride=1)
        self.proj_bottleneck = nn.Linear(self.base * 8 * block.expansion, self.text_dim)
        self.l2norm = Normalize(2)

        self.gnn_gc1 = GraphConvolution(self.text_dim, self.text_dim)
        self.gnn_gc2 = GraphConvolution(self.text_dim, n_class)
        self.gnn_relu1 = nn.LeakyReLU(0.2)

        self.Atext = gen_A2(self.text_features.detach(),ratio=edge_ratio)
        self.text_labels = text_labels.cuda()

        self.text_mean = text_features.mean(dim=1).cuda()

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x, label, w_ti=1.0, w_ii=1.0):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x1 = self.maxpool(x)
        x2 = self.layer1(x1)
        x3 = self.layer2(x2)
        x4 = self.layer3(x3)
        x5 = self.layer4(x4)
        x6 = self.avgpool(x5)
        x6 = x6.view(x6.size(0), -1)
        f = self.proj_bottleneck(x6)

        # forward the auxiliary branch with GCN
        inp = torch.cat((self.text_features, f),dim=0)
        A_all = gen_Aall(self.Atext, self.text_labels, label, w_ti, w_ii)
        adj = gen_adj(A_all).detach()
        g = self.gnn_gc1(inp, adj)
        g = self.gnn_relu1(g)
        y = self.gnn_gc2(g, adj)
        gt = y[self.n_texts:].detach()
        return y[self.n_texts:], g[self.n_texts:], f, x6

    def inference(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x1 = self.maxpool(x)
        x2 = self.layer1(x1)
        x3 = self.layer2(x2)
        x4 = self.layer3(x3)
        x5 = self.layer4(x4)
        x6 = self.avgpool(x5)
        x6 = x6.view(x6.size(0), -1)
        # f = self.proj_bottleneck(x6)
        return x6
    
    def get_similarity(self,features):
        f_n = torch.nn.functional.normalize(features, dim=-1)
        cos_sim = f_n @ f_n.T
        cos_sim.cuda()

        N, C = features.size()
        features_expand1 = features.cpu().repeat(N,1)
        features_expand2 = features.cpu().repeat(1,N).view(-1,C)
        l2_dis = (features_expand1-features_expand2).norm(dim=-1).cuda()
        return cos_sim, l2_dis

    def print_sim_matrix(self, f, label,  dis='cos'):
        if dis=='cos':
            gt = torch.nn.functional.normalize(f, dim=1)
            cos_sim = gt @ gt.T
            print(gt @ gt.T)
            t = self.text_mean[label]
            t = torch.nn.functional.normalize(t, dim=1)
            cos_sim_t = t @ t.T
            print((cos_sim-cos_sim_t).norm(p=1).mean())
        elif dis =='l2':
            N, C = f.size()
            features_expand1 = f.cpu().repeat(N,1)
            features_expand2 = f.cpu().repeat(1,N).view(-1,C)
            l2_dis = (features_expand1-features_expand2).norm(dim=-1).cuda()
            print(l2_dis)

            d = self.text_mean[label]
            N, C = d.size()
            features_expand1 = d.cpu().repeat(N,1)
            features_expand2 = d.cpu().repeat(1,N).view(-1,C)
            l2_dis_d = (features_expand1-features_expand2).norm(dim=-1).cuda()
            print((l2_dis-l2_dis_d).norm(p=1).mean())

class Projector(nn.Module):
    def __init__(self, in_dim, projector_dim):
        super(Projector, self).__init__()
        self.proj_fc1 = nn.Linear(in_dim, projector_dim)
        self.proj_relu = nn.ReLU(inplace=True)
        self.proj_bn = nn.BatchNorm1d(projector_dim)
        self.proj_fc2 = nn.Linear(projector_dim, projector_dim)
    
    def forward(self, x):
        y = self.proj_fc1(x)
        y = self.proj_relu(y)
        y = self.proj_bn(y)
        y = self.proj_fc2(y)
        return y

def resnet18(pretrained=False, **kwargs):
    """Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))
    return model


def resnet34(pretrained=False, **kwargs):
    """Constructs a ResNet-34 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))
    return model


def resnet50(pretrained=False, **kwargs):
    """Constructs a ResNet-50 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']),strict=False)
    return model


def resnet101(pretrained=False, **kwargs):
    """Constructs a ResNet-101 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))
    return model


def resnet152(pretrained=False, **kwargs):
    """Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))
    return model

