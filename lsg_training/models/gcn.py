import torch
import torch.nn as nn
from torch.nn import Parameter
import torch.utils.model_zoo as model_zoo
import math

# Generate adjacency matrix for LSG
def gen_A(features): # generated by threshold
    th = 0.95
    features_norm = features / features.norm(dim=-1,keepdim=True)
    _adj = features_norm @ features_norm.T
    _adj = (_adj - _adj.min())/(_adj.max()-_adj.min())
    similar = (_adj > th).float()
    above = similar.sum()
    below = (1-similar).sum()
    print("ratio-{}:{:.1f}".format(th, above*100/(above+below)),end=' ')
    return similar

def gen_A2(features, ratio=0.002): #v2: generated by ratio
    ratio = 1 - ratio
    features_norm = features / features.norm(dim=-1,keepdim=True)
    _adj = features_norm @ features_norm.T
    _adj_flat = _adj.view(-1)
    th, _ = torch.kthvalue(_adj_flat, int(ratio*_adj_flat.size(0))) #top k-th value of the full similarity matrix
    similar = (_adj >= th).float()
    above = similar.sum()
    below = (1-similar).sum()
    
    print("Connected edges {}, unconnected edges {}, ratio {:.1f}".format(above, below, above*100/(above+below)))
    return similar*_adj

def gen_A2Plus(features, ratio=0.002): #v2Plus: generated by ratio, always keep the edges connecting the same label
    nprompts = 20
    ncls = int(features.size(0)/nprompts) 
    mask = 1 - gen_A_gt(ncls, nprompts)

    ratio = 1 - ratio
    features_norm = features / features.norm(dim=-1,keepdim=True)
    _adj = features_norm @ features_norm.T
    _adj_m = _adj * mask
    _adj_flat = _adj_m.view(-1)
    th, _ = torch.kthvalue(_adj_flat, int(ratio*_adj_flat.size(0))) #top k-th value of the full similarity matrix
    similar = (_adj >= th).float()
    above = similar.sum()
    below = (1-similar).sum()

    print("Connected edges {}, unconnected edges {}, ratio {:.3f}".format(above, below, above*100/(above+below)))
    similar = similar * mask + (1-mask)
    return similar*_adj

def gen_A3(features, ratio=0.002): #v3: ratio + eculidean distance
    beta = 1.0
    ratio = 1 - ratio
    N, C = features.size()
    l2_dis = [] 
    for i in range(N):
        features_expand = features[i].repeat(N,1)
        dis = (features_expand-features).norm(dim=-1)
        l2_dis.append(dis) 
    l2_dis = torch.cat(l2_dis, dim=0)
    _adj = torch.exp(-l2_dis*beta).view(N,N)
    _adj_flat = _adj.view(-1)
    th, _ = torch.kthvalue(_adj_flat, int(ratio*_adj_flat.size(0))) #top k-th value of the full similarity matrix
    similar = (_adj >= th).float()
    above = similar.sum()
    below = (1-similar).sum()

    print("Connected edges {}, unconnected edges {}, ratio {:.1f}".format(above, below, above*100/(above+below)))
    return similar*_adj

def gen_A_gt(ncls,nprompts):
    a = torch.ones(nprompts,nprompts)
    a_all = a.unsqueeze(0).repeat(ncls,1,1)
    return torch.block_diag(*a_all).cuda()


def gen_adj(A):
    D = torch.pow(A.sum(1).float(), -0.5)
    D = torch.diag(D)
    adj = torch.matmul(torch.matmul(A, D).t(), D)
    return adj

class GraphConvolution(nn.Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, bias=False):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.Tensor(in_features, out_features))
        #print("bias",bias)
        if bias:
            self.bias = Parameter(torch.Tensor(1, 1, out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
        #print(self.weight.data)

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = torch.matmul(input, self.weight)
        output = torch.matmul(adj, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

class GCN(nn.Module):

    def __init__(self, text_features, ratio):
        n_class, n_prompt, text_dim = text_features.size()
        #print("size2:", text_features.size())
        self.text_dim = text_dim
        #print(self.text_dim)
        self.text_features = text_features.reshape(-1, self.text_dim).cuda()
        self.ratio = ratio
        super(GCN, self).__init__()

        self.gnn_gc1 = GraphConvolution(self.text_dim, self.text_dim)
        self.gnn_gc2 = GraphConvolution(self.text_dim, n_class)
        self.gnn_relu1 = nn.LeakyReLU(0.2)

        A = gen_A2Plus(self.text_features.detach(), ratio)
        self.adj = gen_adj(A).detach()
        
    def forward(self, _):
        inp = self.text_features
        adj = self.adj
        y = self.gnn_gc1(inp, adj)
        y = self.gnn_relu1(y)
        y = self.gnn_gc2(y, adj)
        y = nn.functional.normalize(y, dim=1)

        return y
        #return y[self.text_features.size(0):], f
    
    def inference(self, t):
        A = gen_A2Plus(t.detach(),self.ratio)
        adj = gen_adj(A).detach()
        y = self.gnn_gc1(t, adj)
        y = self.gnn_relu1(y)
        y = self.gnn_gc2(y, adj)

        return y

    def middle_feature(self, t):
        A = gen_A2Plus(t.detach(),self.ratio)
        adj = gen_adj(A).detach()
        y = self.gnn_gc1(t, adj)
        y = self.gnn_relu1(y)
        return y

